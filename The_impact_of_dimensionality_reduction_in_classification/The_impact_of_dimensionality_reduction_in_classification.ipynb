{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "226e3fdb",
   "metadata": {},
   "source": [
    "# Task 3: The impact of dimensionality reduction in classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d9bd3",
   "metadata": {},
   "source": [
    "The [20newsgroup](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized.html) dataset is a high-dimensional text dataset of scikit-learn, used primarily in classification problems. It includes 18846 news articles from 20 categories. The number of features is 130107, a number that may easily trigger the curse of dimensionality for many machine learning algorithms.\n",
    "\n",
    "Apply PCA for various sizes of the input space (e.g. 50, 100, 500, 1000, 10000, and so on). Compare the performance of LogisticRegression, Random Forest Classifier and Multilayer Perceptron on both the reduced, and original dimensional spaces.\n",
    "\n",
    "* Hint 1: Fetch the dataset in [vectorized format](sklearn.datasets.fetch_20newsgroups_vectorized) and convert it by using the [TfidfTransfomer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html).\n",
    "* Hint 2: In the vast majority of cases, the vectorized text datasets are stored in sparse vectors (namely, most of their components are zero). PCA will not work with such datasets. Use scikit-learn's [TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) instead. TruncatedSVD is similar to PCA, however, it  does not center the data before computing the singular value decomposition. This means it can work with sparse matrices efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c9acc",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
    "This module contains two loaders. The first one, <b>sklearn.datasets.fetch_20newsgroups</b>, returns a list of the raw texts that can be fed to text feature extractors such as <b>CountVectorizer</b> with custom parameters so as to extract feature vectors. The second one, <b>sklearn.datasets.fetch_20newsgroups_vectorized</b>, returns ready-to-use features, i.e., it is not necessary to use a feature extractor."
   ]
  },
  {
   "cell_type": "raw",
   "id": "20cc3c4c",
   "metadata": {},
   "source": [
    "Data Set Characteristics:\n",
    "Classes: 20, Samples: 18846\n",
    "Dimensionality: 1\n",
    "Features: text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups # import 20newsgroups dataset, a list of the raw texts\n",
    "print(type(fetch_20newsgroups)) # <class 'function'>\n",
    "fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd01d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The F-score will be lower because it is more realistic.\n",
    "# newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories=categories)\n",
    "newsgroups_train = fetch_20newsgroups(subset='train') # store features train dataset\n",
    "print(type(newsgroups_train))\n",
    "print(len(newsgroups_train.data))\n",
    "newsgroups_train     #11314 articles/samples of the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc57c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test') # store features test dataset\n",
    "print(type(newsgroups_test))\n",
    "print(len(newsgroups_test.data))    \n",
    "newsgroups_test   # 7532 articles/samples of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34208efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(newsgroups_train.target_names))   \n",
    "print(newsgroups_train.target_names) # map numbers to categories  (20 classes-topics of the train dataset)\n",
    "print(newsgroups_train.target.shape) # (11314,)\n",
    "print(newsgroups_train.target[:10])  # 10 articles of the train dataset with their corresponding category   \n",
    "print(\"\\n11314 articles of the train dataset with their corresponding category\")\n",
    "print(list(newsgroups_train.target[:11314]))  # 11314 articles of the train dataset with their corresponding category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87882a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting text to vectors of numerical values suitable for statistical analysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # tf-idf counts the frequency of a word in the document\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(newsgroups_train.data)\n",
    "print(type(X_train)) # scipy.sparse.csr.csr_matrix\n",
    "print(X_train.shape) # samples-aticles(rows):11314, distinct words/features(columns):130107\n",
    "y_train = newsgroups_train.target  # 11314 articles of the train dataset with their corresponding category\n",
    "print(type(y_train)) # <class 'numpy.ndarray'>\n",
    "print(y_train.shape) # (11314,)\n",
    "print(\"\\nX_train=\",X_train)\n",
    "print(\"\\ny_train=\",y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7183794",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf.transform(newsgroups_test.data)\n",
    "print(type(X_test)) # <class 'scipy.sparse.csr.csr_matrix'>\n",
    "print(X_test.shape) # (7532 posts, 130107 distinct words)\n",
    "y_test = newsgroups_test.target\n",
    "print(type(y_test)) # <class 'numpy.ndarray'>\n",
    "print(y_test.shape) # (7532,)\n",
    "print(\"\\nX_test=\",X_test)\n",
    "print(\"\\ny_test=\",y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tfidf.vocabulary_))\n",
    "tfidf.vocabulary_ # word:#appearances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb10a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The extracted TF-IDF vectors are very sparse\n",
    "print(X_train.nnz) # not zero terms\n",
    "print(round(X_train.nnz / float(X_train.shape[0]))) # avg of 158 non-zero components by sample of 130107-dimensional space-post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f14a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.asarray(np.unique(y_train, return_counts=True)).T # [Class:#News] (20 categories with their corresponding # of articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9012b1e",
   "metadata": {},
   "source": [
    "### Ready to run our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3170a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py\n",
    "import numpy as np, pandas as pd, os, sys, scipy.io, logging\n",
    "from optparse import OptionParser\n",
    "from time import time\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c9075",
   "metadata": {},
   "source": [
    "## Truncated Singular Value Decomposition (TSVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD as TSVD    # Dimensionality reduction using truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30792f93",
   "metadata": {},
   "source": [
    "## Dimensional reduction to 50, 100, 500, 1000, 10000, 20000, 50000, 100000 (TSVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48266092",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = []\n",
    "for c in [50,100]: #,500,1000,10000, 20000, 50000, 100000\n",
    "    print(\"\\nFor TSVD({}):\".format(c))\n",
    "    # run many diff Truncated Singular Value Decomposition (TSVD)\n",
    "    tsvd = TSVD(n_components=c, random_state=0)                    #dimensional reduction, convert matrix from 130.107 columns to c columns \n",
    "    X_train_tsvd = tsvd.fit_transform(X_train)\n",
    "    X_test_tsvd = tsvd.transform(X_test)         # perform dimensionality reduction to the X_test dataset\n",
    "    print('Shape of X train is',X_train.shape, '\\nReduced shape of X_train is',X_train_tsvd.shape) # original vs reduced dimensional space on X_train\n",
    "    print('\\nThe shape of y_train is: ',y_train.shape)\n",
    "    print('\\nShape of X_test is',X_test.shape, '\\nReduced shape of X_test is',X_test_tsvd.shape) # original vs reduced dimensional space on X_test   \n",
    "    print('\\nThe shape of y_test is: ',y_test.shape)\n",
    "    print('\\nTotal variance of X_train is',sum(tsvd.explained_variance_))  # what percentage of the previous training matrix does this matrix represent\n",
    "    print('Total percentage variance of X_train is',sum(tsvd.explained_variance_ratio_))\n",
    "    print('\\nVariance of the training samples=',tsvd.explained_variance_)     #The variance of the training samples transformed by a projection to each component (what percentage of the previous columns does these new columns represent).\n",
    "    print('\\nPercentage variance of the training samples=',tsvd.explained_variance_ratio_)  #Percentage of variance explained by each of the selected components.    \n",
    "    scores.append(tsvd.explained_variance_ratio_.sum()) # store all scores\n",
    "print('\\nPercentage variance on the train subset for c=50,100,500,1000,10000,20000,50000,100000 is',scores)\n",
    "\n",
    "#MemoryError: Unable to allocate 19.4 GiB for an array with shape (130107, 20010) and data type float64\n",
    "#MemoryError: Unable to allocate 48.5 GiB for an array with shape (130107, 50010) and data type float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed557c1d",
   "metadata": {},
   "source": [
    "### Run a simple classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204e4bb",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "**Regularization**:\n",
    "- prevention of overfitting - (according to Muller and Guido ML book)\n",
    "- L1 - assumes only a few features are important\n",
    "- L2 - does not assume only a few features are important - used by default in scikit-learn LogisticRegression\n",
    "               \n",
    "**'C'**:\n",
    "- parameter to control the strength of regularization\n",
    "- lower C => log_reg adjusts to the majority of data points.\n",
    "- higher C => correct classification of each data point.\n",
    "\n",
    "C is known as the alpha parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f969d4c7",
   "metadata": {},
   "source": [
    "Defaults of Logistic Regression (penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Logistic Regression without dimensional reduction\n",
    "log_reg = LogisticRegression() # penalty='l2', C=1, solver='lbfgs', max_iter=100\n",
    "log_reg.fit(X_train, y_train)\n",
    "lr_pred = log_reg.predict(X_test)  #predicted values for x_test\n",
    "print('The accuracy score for logistic regression is: ',accuracy_score(y_test, lr_pred))   #y_true,Y_predicted\n",
    "print('\\nThe predicted values of the X_test is: ',lr_pred)\n",
    "print('\\nThe shape of the predicted values for the X_test is: ',lr_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dfd20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on the training subset: {:.3f}'.format(log_reg.score(X_train, y_train)))\n",
    "print('Accuracy on the test subset: {:.3f}'.format(log_reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f62c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(y_test, lr_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8d45b",
   "metadata": {},
   "source": [
    "<b>Logistic Regression with dimensional reduction to 50,100,500,1000, 10000 columns</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec1bee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "classifier = LogisticRegression()\n",
    "scores = []\n",
    "for c in [50, 100]: #, 500, 1000, 10000, 20000, 50000, 100000\n",
    "    print(\"For TSVD({}):\".format(c))\n",
    "    # run many diff Truncated Singular Value Decomposition (TSVD)\n",
    "    tsvd = TSVD(n_components=c, random_state=0)                    #dimensional reduction \n",
    "    X_train_tsvd = tsvd.fit_transform(X_train)\n",
    "    X_test_tsvd = tsvd.transform(X_test)\n",
    "    ### Logistic Regression with dimensional reduction     \n",
    "    log_reg = LogisticRegression() # penalty='l2', C=1, solver='lbfgs', max_iter=100\n",
    "    log_reg.fit(X_train_tsvd, y_train)\n",
    "    lr_pred = log_reg.predict(X_test_tsvd)   #predicted values for x_test\n",
    "    # accuracy_score(y_test, lr_pred)\n",
    "    print('Accuracy on the training subset: {:.3f}'.format(log_reg.score(X_train_tsvd, y_train)))\n",
    "    print('Accuracy on the test subset: {:.3f}'.format(log_reg.score(X_test_tsvd, y_test)))\n",
    "    scores.append(log_reg.score(X_test_tsvd, y_test)) # store all scores\n",
    "    #print(confusion_matrix(y_test, lr_pred))\n",
    "    report=classification_report(y_test, lr_pred)\n",
    "    print(report)\n",
    "    \n",
    "print('\\nAccuracy on the test subset for c=50,100,500,1000,10000,20000,50000,100000 is',scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a85c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the statistics\n",
    "print('Logistic regression with dimensional reduction')\n",
    "print('\\nFor TSVD(50):')\n",
    "print('Accuracy on the test subset: 0.67')\n",
    "print('Average precision between true and predicted values for the test subject: 0.67')\n",
    "print('Average recall between true and predicted values for the test subject: 0.66')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.65')\n",
    "print('\\nFor TSVD(100):')\n",
    "print('Accuracy on the test subset: 0.72')\n",
    "print('Average precision between true and predicted values for the test subject: 0.72')\n",
    "print('Average recall between true and predicted values for the test subject: 0.71')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.71')\n",
    "print('\\nFor TSVD(500):')\n",
    "print('Accuracy on the test subset: 0.79')\n",
    "print('Average precision between true and predicted values for the test subject: 0.79')\n",
    "print('Average recall between true and predicted values for the test subject: 0.78')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.78')\n",
    "print('\\nFor TSVD(1000):')\n",
    "print('Accuracy on the test subset: 0.80')\n",
    "print('Average precision between true and predicted values for the test subject: 0.81')\n",
    "print('Average recall between true and predicted values for the test subject: 0.79')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.79')\n",
    "print('\\nFor TSVD(10000):')\n",
    "print('Accuracy on the test subset: 0.83')\n",
    "print('Average precision between true and predicted values for the test subject: 0.83')\n",
    "print('Average recall between true and predicted values for the test subject: 0.82')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.82')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9be35e",
   "metadata": {},
   "source": [
    "Comparing the average for the metrics of Precision, Recall, F1-score and Accuracy , it can be concluded that the metrics are better when the dimension is increased. Moreover, comparing the first statistics of logistic regression with the statistics of dimensional reduction to 10.000 dimensions , it can be concluded that they are similar, thus there is not a significant loss of information. So, logistic regression with dimensional reduction to 10.000 is a great represantation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f17a6",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### Random Forest with no dimensional reduction\n",
    "rf = RandomForestClassifier() #defaults: n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)    #predicted values for x_test\n",
    "print('Accuracy between the true and predicted values for test subject is: ',accuracy_score(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b606f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on the training subset: {:.3f}'.format(rf.score(X_train, y_train)))\n",
    "print('Accuracy on the test subset: {:.3f}'.format(rf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df5cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(y_test, rf_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58c58b",
   "metadata": {},
   "source": [
    "<b>Random Forest with dimensional reduction to 50,100,500,1000, 10000 columns</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190910d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# run with Truncated Singular Value Decomposition (TSVD)\n",
    "classifier = RandomForestClassifier()\n",
    "scores = []\n",
    "for c in [50,100]: #, 500, 1000, 10000, 20000, 50000, 100000\n",
    "    print(\"For TSVD({}):\".format(c))\n",
    "    # run many diff Truncated Singular Value Decomposition (TSVD)\n",
    "    tsvd = TSVD(n_components=c, random_state=0)                    #dimensional reduction \n",
    "    X_train_tsvd = tsvd.fit_transform(X_train)\n",
    "    X_test_tsvd = tsvd.transform(X_test)\n",
    "    ### Random Forest with dimensional reduction     \n",
    "    rf = RandomForestClassifier()  # defaults: n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1\n",
    "    rf.fit(X_train_tsvd, y_train)\n",
    "    rf_pred = rf.predict(X_test_tsvd) #predicted values for x_test\n",
    "    # accuracy_score(y_test, lr_pred)\n",
    "    print('Accuracy on the training subset: {:.3f}'.format(rf.score(X_train_tsvd, y_train)))\n",
    "    print('Accuracy on the test subset: {:.3f}'.format(rf.score(X_test_tsvd, y_test)))\n",
    "    scores.append(rf.score(X_test_tsvd, y_test)) # store all scores\n",
    "    report=classification_report(y_test, rf_pred)\n",
    "    print(report)   \n",
    "print('\\nAccuracy on the test subset for c=50,100,500,1000,10000,20000,50000,100000 is',scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the statistics\n",
    "print('Random Forest with dimensional reduction')\n",
    "print('\\nFor TSVD(50):')\n",
    "print('Accuracy on the test subset: 0.65')\n",
    "print('Average precision between true and predicted values for the test subject: 0.65')\n",
    "print('Average recall between true and predicted values for the test subject: 0.64')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.64')\n",
    "print('\\nFor TSVD(100):')\n",
    "print('Accuracy on the test subset: 0.67')\n",
    "print('Average precision between true and predicted values for the test subject: 0.68')\n",
    "print('Average recall between true and predicted values for the test subject: 0.66')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.66')\n",
    "print('\\nFor TSVD(500):')\n",
    "print('Accuracy on the test subset: 0.67')\n",
    "print('Average precision between true and predicted values for the test subject: 0.68')\n",
    "print('Average recall between true and predicted values for the test subject: 0.66')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.66')\n",
    "print('\\nFor TSVD(1000):')\n",
    "print('Accuracy on the test subset: 0.67')\n",
    "print('Average precision between true and predicted values for the test subject: 0.68')\n",
    "print('Average recall between true and predicted values for the test subject: 0.66')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.66')\n",
    "print('\\nFor TSVD(10000):')\n",
    "print('Accuracy on the test subset: 0.58')\n",
    "print('Average precision between true and predicted values for the test subject: 0.59')\n",
    "print('Average recall between true and predicted values for the test subject: 0.57')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.56')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dbc44e",
   "metadata": {},
   "source": [
    "The interesting point in this case is that when the dimensions are decreased to 100,500 and 1.000 , we have a good enough score (accuracy of 0.67,hence just 10% loss from the original dimensions) while taking into account this massive change-reduction in the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d5bf1",
   "metadata": {},
   "source": [
    "### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "# mlp = MLPClassifier(solver='adam', activation='relu',alpha=1e-4,hidden_layer_sizes=(100,), random_state=1,max_iter=200,verbose=10,learning_rate_init=.001)\n",
    "mlp = MLPClassifier(random_state=1, hidden_layer_sizes=(50,))\n",
    "mlp.fit(X_train, y_train)\n",
    "mlp_pred = mlp.predict(X_test)\n",
    "print(accuracy_score(y_test, mlp_pred))\n",
    "print (mlp.n_layers_)\n",
    "print (mlp.n_iter_)\n",
    "print (mlp.loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b0b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on the training subset: {:.3f}'.format(mlp.score(X_train, y_train)))\n",
    "print('Accuracy on the test subset: {:.3f}'.format(mlp.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(y_test, mlp_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a986ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# mlp = MLPClassifier(solver='adam', activation='relu',alpha=1e-4,hidden_layer_sizes=(100,), random_state=1,max_iter=200,verbose=10,learning_rate_init=.001)\n",
    "classifier = MLPClassifier()\n",
    "scores = []\n",
    "for c in [50, 100]: #, 500, 1000, 10000,  20000, 50000, 100000\n",
    "    print(\"For TSVD({}):\".format(c))\n",
    "    # run many diff Truncated Singular Value Decomposition (TSVD)\n",
    "    tsvd = TSVD(n_components=c, random_state=0)   #dimensional reduction \n",
    "    X_train_tsvd = tsvd.fit_transform(X_train)\n",
    "    X_test_tsvd = tsvd.transform(X_test)\n",
    "    \n",
    "    mlp = MLPClassifier(random_state=1, hidden_layer_sizes=(50,))\n",
    "    mlp.fit(X_train_tsvd, y_train)\n",
    "    mlp_pred = mlp.predict(X_test_tsvd)\n",
    "    print(accuracy_score(y_test, mlp_pred))\n",
    "    print('Accuracy on the training subset: {:.3f}'.format(mlp.score(X_train_tsvd, y_train)))\n",
    "    print('Accuracy on the test subset: {:.3f}'.format(mlp.score(X_test_tsvd, y_test)))\n",
    "    mlp.score(X_test_tsvd, y_test)\n",
    "    scores.append(mlp.score(X_test_tsvd, y_test))\n",
    "    report=classification_report(y_test,mlp_pred)\n",
    "    print(report)\n",
    "print('\\nAccuracy on the test subset for c=50,100,500,1000,10000,20000,50000,100000 is',scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7252352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the statistics\n",
    "print('Multi-layer Perceptron with dimensional reduction')\n",
    "print('\\nFor TSVD(50):')\n",
    "print('Accuracy on the test subset: 0.71')\n",
    "print('Average precision between true and predicted values for the test subject: 0.71')\n",
    "print('Average recall between true and predicted values for the test subject: 0.70')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.70')\n",
    "print('\\nFor TSVD(100):')\n",
    "print('Accuracy on the test subset: 0.75')\n",
    "print('Average precision between true and predicted values for the test subject: 0.74')\n",
    "print('Average recall between true and predicted values for the test subject: 0.74')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.74')\n",
    "print('\\nFor TSVD(500):')\n",
    "print('Accuracy on the test subset: 0.78')\n",
    "print('Average precision between true and predicted values for the test subject: 0.78')\n",
    "print('Average recall between true and predicted values for the test subject: 0.78')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.78')\n",
    "print('\\nFor TSVD(1000):')\n",
    "print('Accuracy on the test subset: 0.8')\n",
    "print('Average precision between true and predicted values for the test subject: 0.8')\n",
    "print('Average recall between true and predicted values for the test subject: 0.8')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.8')\n",
    "print('\\nFor TSVD(10000):')\n",
    "print('Accuracy on the test subset: 0.84')\n",
    "print('Average precision between true and predicted values for the test subject: 0.84')\n",
    "print('Average recall between true and predicted values for the test subject: 0.83')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.83')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb5b7fb",
   "metadata": {},
   "source": [
    "Comparing the average for the metrics of Precision, Recall, F1-score and Accuracy , it can be concluded that the metrics are better when the dimension is increased. We see that from 10.000 features we have gotten close to the accuracy of the original dataset's accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba5fb7",
   "metadata": {},
   "source": [
    "<b>These final metrics are better from the previous ones,so the best model so far is Multi-layer Perceptron with dimensional reduction.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f84ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = MLPClassifier()\n",
    "scores = []\n",
    "for c in [50, 100]: #, 500, 1000, 10000,  20000, 50000, 100000\n",
    "    print(\"For TSVD({}):\".format(c))\n",
    "    # run many diff Truncated Singular Value Decomposition (TSVD)\n",
    "    tsvd = TSVD(n_components=c, random_state=0)   #dimensional reduction \n",
    "    X_train_tsvd = tsvd.fit_transform(X_train)\n",
    "    X_test_tsvd = tsvd.transform(X_test)\n",
    "    \n",
    "    # run multi-submodels for same TSVD(c)\n",
    "    pipe = Pipeline([('classifier', classifier)])\n",
    "    params = {\n",
    "    'classifier__hidden_layer_sizes': [(100,), (100, 10)], # , (80, 15)  ,(100, 10) differentions of the model   \n",
    "    # 'classifier__activation': ['relu', 'tanh', 'logistic'],\n",
    "    'classifier__learning_rate_init': [0.001, 0.01] # 0.0001,                     \n",
    "    # 'classifier__solver': ['sgd', 'adam']\n",
    "    }\n",
    "    grid = GridSearchCV(pipe, params, cv=2, verbose=1, n_jobs=8)\n",
    "    print(grid.fit(X_train_tsvd, y_train))\n",
    "    grid_preds = grid.predict(X_test_tsvd)\n",
    "    # print(grid_preds)\n",
    "    print()\n",
    "    print(\"accuracy_score:\", accuracy_score(y_test, grid_preds))\n",
    "    # print(grid.best_estimator_), print(\"\")\n",
    "    print(\"best_params_:\", grid.best_params_)\n",
    "    # print(pipe.steps)\n",
    "    means = grid.cv_results_['mean_test_score']\n",
    "    for mean, params in zip(means, grid.cv_results_['params']):\n",
    "        print('%0.3f for %r' % (mean, params))\n",
    "        scores.append((mean, params)) # store all scores\n",
    "    report=classification_report(y_test, grid_preds)\n",
    "    print(report)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the statistics\n",
    "print('Multi-layer Perceptron with dimensional reduction,pipeline and grid search')\n",
    "print('\\nFor TSVD(50):')\n",
    "print('Accuracy on the test subset: 0.71')\n",
    "print('Average precision between true and predicted values for the test subject: 0.71')\n",
    "print('Average recall between true and predicted values for the test subject: 0.70')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.70')\n",
    "print('\\nFor TSVD(100):')\n",
    "print('Accuracy on the test subset: 0.75')\n",
    "print('Average precision between true and predicted values for the test subject: 0.74')\n",
    "print('Average recall between true and predicted values for the test subject: 0.74')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.74')\n",
    "print('\\nFor TSVD(500):')\n",
    "print('Accuracy on the test subset: 0.79')\n",
    "print('Average precision between true and predicted values for the test subject: 0.78')\n",
    "print('Average recall between true and predicted values for the test subject: 0.78')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.78')\n",
    "print('\\nFor TSVD(1000):')\n",
    "print('Accuracy on the test subset: 0.80')\n",
    "print('Average precision between true and predicted values for the test subject: 0.79')\n",
    "print('Average recall between true and predicted values for the test subject: 0.79')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.79')\n",
    "print('\\nFor TSVD(10000):')\n",
    "print('Accuracy on the test subset: 0.73')\n",
    "print('Average precision between true and predicted values for the test subject: 0.76')\n",
    "print('Average recall between true and predicted values for the test subject: 0.72')\n",
    "print('Average F1-score between true and predicted values for the test subject: 0.73')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe0fba",
   "metadata": {},
   "source": [
    "Multi-layer Perceptron with dimensional reduction,pipeline and grid search does not have better results.It seams that 50 hidden layers are good enough parameter for this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70b79c",
   "metadata": {},
   "source": [
    "<b>Comparing all models, it is clear that the best model for dimensional reduction is Multi-layer Perceptron with 50 hidden layers.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee6aea",
   "metadata": {},
   "source": [
    "The above results with reduction to 10.000 were managed to be done by allocating more memory (changing the pagefile on windows). However, running with 10.000 and more dimensions will cause memory problems, thus run out of memory and will not be able to operate the above computations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
